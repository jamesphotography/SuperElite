#!/usr/bin/env python3
"""
æµ‹è¯• One-Align çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›
å°è¯•ç»•è¿‡ score() æ–¹æ³•ï¼Œç›´æ¥ç”¨æ¨¡å‹åšåˆ†ç±»/æè¿°
"""

import os
import sys
import time
from pathlib import Path
from PIL import Image
import torch

sys.path.insert(0, str(Path(__file__).parent))
from raw_converter import is_raw_file, raw_to_jpeg
from one_align_scorer import OneAlignScorer


def expand2square(pil_img, background_color):
    """å°†å›¾ç‰‡å¡«å……ä¸ºæ­£æ–¹å½¢"""
    width, height = pil_img.size
    if width == height:
        return pil_img
    elif width > height:
        result = Image.new(pil_img.mode, (width, width), background_color)
        result.paste(pil_img, (0, (width - height) // 2))
        return result
    else:
        result = Image.new(pil_img.mode, (height, height), background_color)
        result.paste(pil_img, ((height - width) // 2, 0))
        return result


def main():
    test_dir = "/Users/jameszhenyu/Desktop/NEWTEST/4"
    
    # æ‰¾ JPG æ–‡ä»¶æµ‹è¯•
    jpg_file = None
    for f in os.listdir(test_dir):
        if f.lower().endswith('.jpg'):
            jpg_file = os.path.join(test_dir, f)
            break
    
    if not jpg_file:
        print("âŒ æœªæ‰¾åˆ° JPG æµ‹è¯•å›¾ç‰‡")
        return
    
    print(f"\nğŸ“· æµ‹è¯•å›¾ç‰‡: {os.path.basename(jpg_file)}\n")
    
    # åŠ è½½æ¨¡å‹
    scorer = OneAlignScorer()
    scorer.load_model()
    model = scorer.model
    
    print(f"\n{'='*70}")
    print("ğŸ” æ£€æŸ¥æ¨¡å‹æ–¹æ³•")
    print(f"{'='*70}")
    
    methods_to_check = ['generate', 'forward', 'chat', 'tokenizer', 'image_processor']
    for method in methods_to_check:
        has_method = hasattr(model, method)
        print(f"  {method}: {'âœ…' if has_method else 'âŒ'}")
    
    # åŠ è½½å›¾ç‰‡
    image = Image.open(jpg_file).convert("RGB")
    
    print(f"\n{'='*70}")
    print("ğŸ§ª æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ")
    print(f"{'='*70}")
    
    # å‡†å¤‡å›¾åƒå¼ é‡
    image_squared = expand2square(image, tuple(int(x*255) for x in model.image_processor.image_mean))
    image_tensor = model.image_processor.preprocess([image_squared], return_tensors="pt")["pixel_values"].half().to(model.device)
    
    # IMAGE_TOKEN_INDEX é€šå¸¸æ˜¯ -200
    IMAGE_TOKEN_INDEX = -200
    
    def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):
        """ç®€åŒ–ç‰ˆçš„ tokenizer_image_token"""
        prompt_chunks = prompt.split('<|image|>')
        input_ids = []
        
        for i, chunk in enumerate(prompt_chunks):
            chunk_ids = tokenizer.encode(chunk, add_special_tokens=(i == 0))
            input_ids.extend(chunk_ids)
            if i < len(prompt_chunks) - 1:
                input_ids.append(image_token_index)
        
        if return_tensors == 'pt':
            return torch.tensor(input_ids, dtype=torch.long)
        return input_ids
    
    # å‡†å¤‡æµ‹è¯•æç¤º
    test_prompts = [
        # åˆ†ç±»ä»»åŠ¡
        ("åœºæ™¯åˆ†ç±» (EN)", 
         "USER: Classify this landscape photo into one category: sunset, sunrise, aurora, night, waterfall, mountain, seascape, cityscape, forest, desert. Answer with one word only.\n<|image|>\nASSISTANT:"),
        
        ("åœºæ™¯åˆ†ç±» (ä¸­æ–‡)", 
         "USER: è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„é£å…‰ç…§ç‰‡ï¼Ÿè¯·ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªï¼šæ—¥è½ã€æ—¥å‡ºã€æå…‰ã€å¤œæ™¯ã€ç€‘å¸ƒã€å±±æ™¯ã€æµ·æ™¯ã€åŸå¸‚ã€æ£®æ—ã€æ²™æ¼ ã€‚åªå›ç­”ä¸€ä¸ªè¯ã€‚\n<|image|>\nASSISTANT:"),
        
        ("æ—¶é—´åˆ¤æ–­", 
         "USER: What time of day was this photo taken? Answer: golden hour, blue hour, midday, night, or overcast.\n<|image|>\nASSISTANT:"),
        
        ("æƒ…ç»ªåˆ¤æ–­", 
         "USER: What is the mood of this photograph? Answer with one or two words.\n<|image|>\nASSISTANT:"),
        
        # æè¿°ä»»åŠ¡
        ("ç®€çŸ­æè¿° (EN)", 
         "USER: Describe this photograph in one sentence.\n<|image|>\nASSISTANT:"),
        
        ("ç®€çŸ­æè¿° (ä¸­æ–‡)", 
         "USER: ç”¨ä¸€å¥è¯æè¿°è¿™å¼ ç…§ç‰‡ã€‚\n<|image|>\nASSISTANT:"),
        
        # æ›´è¯¦ç»†çš„åˆ†æ
        ("ä¼˜ç‚¹åˆ†æ", 
         "USER: What are the main strengths of this photograph?\n<|image|>\nASSISTANT:"),
        
        ("æ”¹è¿›å»ºè®®", 
         "USER: What could be improved in this photograph?\n<|image|>\nASSISTANT:"),
    ]
    
    for name, prompt in test_prompts:
        print(f"\n--- {name} ---")
        
        start = time.time()
        
        try:
            # Tokenize prompt
            input_ids = tokenizer_image_token(prompt, model.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)
            
            # å°è¯• generate
            with torch.inference_mode():
                output_ids = model.generate(
                    input_ids,
                    images=image_tensor,
                    max_new_tokens=100,
                    do_sample=False,
                    num_beams=1,
                    use_cache=True,
                )
            
            # è§£ç è¾“å‡º
            output_text = model.tokenizer.decode(output_ids[0], skip_special_tokens=True)
            
            # æå– ASSISTANT ä¹‹åçš„éƒ¨åˆ†
            if "ASSISTANT:" in output_text:
                response = output_text.split("ASSISTANT:")[-1].strip()
            else:
                response = output_text
            
            elapsed = time.time() - start
            print(f"  ğŸ“ Response: {response}")
            print(f"  â±ï¸  Time: {elapsed:.2f}s")
            
        except Exception as e:
            elapsed = time.time() - start
            print(f"  âŒ Error: {e}")
            print(f"  â±ï¸  Time: {elapsed:.2f}s")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()
