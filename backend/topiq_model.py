"""
ç‹¬ç«‹ TOPIQ (Top-down Image Quality Assessment) æ¨¡å‹å®ç°
è„±ç¦» pyiqa æ¡†æ¶è¿è¡Œï¼Œç”¨äºé¸Ÿç±»æ‘„å½±ç¾å­¦è¯„åˆ†

åŸºäº:
- TOPIQ: A Top-down Approach from Semantics to Distortions for Image Quality Assessment
- Chaofeng Chen et al., IEEE TIP 2024
- åŸå§‹å®ç°: https://github.com/chaofengc/IQA-PyTorch

å…³é”®ä¼˜åŠ¿:
- ä»è¯­ä¹‰åˆ°å¤±çœŸçš„ Top-down æ–¹æ³•
- å¯¹ç”»é¢ä¸»ä½“ï¼ˆå¦‚é¸Ÿç±»ï¼‰æœ‰æ›´å¥½çš„è¯­ä¹‰ç†è§£
- ä½¿ç”¨ ResNet50 éª¨å¹²ï¼Œæ¯” NIMA çš„ InceptionResNetV2 æ›´è½»é‡

ä¾èµ–é¡¹: torch, timm, PIL
"""

import os
import sys
import copy
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torchvision.transforms.functional as TF
import torchvision.transforms as T
from PIL import Image
from collections import OrderedDict

import timm


# ImageNet æ ‡å‡†åŒ–å‚æ•°
IMAGENET_DEFAULT_MEAN = [0.485, 0.456, 0.406]
IMAGENET_DEFAULT_STD = [0.229, 0.224, 0.225]


def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])


def _get_activation_fn(activation):
    """Return an activation function given a string"""
    if activation == 'relu':
        return F.relu
    if activation == 'gelu':
        return F.gelu
    if activation == 'glu':
        return F.glu
    raise RuntimeError(f'activation should be relu/gelu, not {activation}.')


def dist_to_mos(dist_score: torch.Tensor) -> torch.Tensor:
    """
    Convert distribution prediction to MOS score.
    For datasets with detailed score labels, such as AVA.
    
    Args:
        dist_score: (*, C), C is the class number.
        
    Returns:
        (*, 1) MOS score.
    """
    num_classes = dist_score.shape[-1]
    mos_score = dist_score * torch.arange(1, num_classes + 1).to(dist_score)
    mos_score = mos_score.sum(dim=-1, keepdim=True)
    return mos_score


class TransformerEncoderLayer(nn.Module):
    def __init__(
        self,
        d_model,
        nhead,
        dim_feedforward=2048,
        dropout=0.1,
        activation='gelu',
        normalize_before=False,
    ):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def forward(self, src):
        src2 = self.norm1(src)
        q = k = src2
        src2, self.attn_map = self.self_attn(q, k, value=src2)
        src = src + self.dropout1(src2)
        src2 = self.norm2(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))
        src = src + self.dropout2(src2)
        return src


class TransformerDecoderLayer(nn.Module):
    def __init__(
        self,
        d_model,
        nhead,
        dim_feedforward=2048,
        dropout=0.1,
        activation='gelu',
        normalize_before=False,
    ):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def forward(self, tgt, memory):
        memory = self.norm2(memory)
        tgt2 = self.norm1(tgt)
        tgt2, self.attn_map = self.multihead_attn(query=tgt2, key=memory, value=memory)
        tgt = tgt + self.dropout2(tgt2)
        tgt2 = self.norm3(tgt)
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))
        tgt = tgt + self.dropout3(tgt2)
        return tgt


class TransformerEncoder(nn.Module):
    def __init__(self, encoder_layer, num_layers):
        super().__init__()
        self.layers = _get_clones(encoder_layer, num_layers)
        self.num_layers = num_layers

    def forward(self, src):
        output = src
        for layer in self.layers:
            output = layer(output)
        return output


class TransformerDecoder(nn.Module):
    def __init__(self, decoder_layer, num_layers):
        super().__init__()
        self.layers = _get_clones(decoder_layer, num_layers)
        self.num_layers = num_layers

    def forward(self, tgt, memory):
        output = tgt
        for layer in self.layers:
            output = layer(output, memory)
        return output


class GatedConv(nn.Module):
    def __init__(self, weightdim, ksz=3):
        super().__init__()

        self.splitconv = nn.Conv2d(weightdim, weightdim * 2, 1, 1, 0)
        self.act = nn.GELU()

        self.weight_blk = nn.Sequential(
            nn.Conv2d(weightdim, 64, 1, stride=1),
            nn.GELU(),
            nn.Conv2d(64, 64, ksz, stride=1, padding=1),
            nn.GELU(),
            nn.Conv2d(64, 1, ksz, stride=1, padding=1),
            nn.Sigmoid(),
        )

    def forward(self, x):
        x1, x2 = self.splitconv(x).chunk(2, dim=1)
        weight = self.weight_blk(x2)
        x1 = self.act(x1)
        return x1 * weight


class CFANet(nn.Module):
    """
    TOPIQ CFANet æ¨¡å‹ - ç¾å­¦è¯„åˆ† (IAA)
    
    ä½¿ç”¨ ResNet50 ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œé€šè¿‡è·¨å°ºåº¦æ³¨æ„åŠ›æœºåˆ¶
    å®ç°ä»è¯­ä¹‰åˆ°å¤±çœŸçš„ä»ä¸Šåˆ°ä¸‹çš„è´¨é‡è¯„ä¼°ã€‚
    
    å¯¹äºé¸Ÿç±»æ‘„å½±ï¼Œè¿™ç§æ–¹æ³•èƒ½æ›´å¥½åœ°ç†è§£ç”»é¢ä¸»ä½“ï¼Œ
    å› ä¸ºå®ƒé¦–å…ˆè¯†åˆ«è¯­ä¹‰ä¿¡æ¯ï¼ˆé¸Ÿçš„ä½ç½®ã€å§¿æ€ï¼‰ï¼Œ
    ç„¶åå†è¯„ä¼°ç»†èŠ‚è´¨é‡ã€‚
    """
    
    def __init__(
        self,
        semantic_model_name='resnet50',
        backbone_pretrain=False,  # æˆ‘ä»¬ä¼šåŠ è½½å®Œæ•´æƒé‡ï¼Œä¸éœ€è¦ ImageNet é¢„è®­ç»ƒ
        use_ref=False,  # NR-IQA æ¨¡å¼ï¼ˆæ— å‚è€ƒï¼‰
        num_class=10,   # AVA æ•°æ®é›†ä½¿ç”¨ 10 åˆ†ç±»
        inter_dim=512,
        num_heads=4,
        num_attn_layers=1,
        dprate=0.1,
        activation='gelu',
        test_img_size=None,
    ):
        super().__init__()

        self.semantic_model_name = semantic_model_name
        self.use_ref = use_ref
        self.num_class = num_class
        self.test_img_size = test_img_size

        # =============================================================
        # ResNet50 éª¨å¹²ç½‘ç»œï¼ˆä»…æå–ç‰¹å¾ï¼‰
        # =============================================================
        self.semantic_model = timm.create_model(
            semantic_model_name, pretrained=backbone_pretrain, features_only=True
        )
        feature_dim_list = self.semantic_model.feature_info.channels()
        
        # ImageNet å½’ä¸€åŒ–å‚æ•°
        self.default_mean = torch.Tensor(IMAGENET_DEFAULT_MEAN).view(1, 3, 1, 1)
        self.default_std = torch.Tensor(IMAGENET_DEFAULT_STD).view(1, 3, 1, 1)

        # =============================================================
        # è‡ªæ³¨æ„åŠ›å’Œè·¨å°ºåº¦æ³¨æ„åŠ›æ¨¡å—
        # =============================================================
        ca_layers = sa_layers = num_attn_layers
        self.act_layer = nn.GELU() if activation == 'gelu' else nn.ReLU()
        dim_feedforward = min(4 * inter_dim, 2048)

        # é—¨æ§å±€éƒ¨æ± åŒ–å’Œè‡ªæ³¨æ„åŠ›
        tmp_layer = TransformerEncoderLayer(
            inter_dim,
            nhead=num_heads,
            dim_feedforward=dim_feedforward,
            normalize_before=True,
            dropout=dprate,
            activation=activation,
        )
        
        self.sa_attn_blks = nn.ModuleList()
        self.dim_reduce = nn.ModuleList()
        self.weight_pool = nn.ModuleList()
        
        for idx, dim in enumerate(feature_dim_list):
            self.weight_pool.append(GatedConv(dim))
            self.dim_reduce.append(
                nn.Sequential(
                    nn.Conv2d(dim, inter_dim, 1, 1),
                    self.act_layer,
                )
            )
            self.sa_attn_blks.append(TransformerEncoder(tmp_layer, sa_layers))

        # è·¨å°ºåº¦æ³¨æ„åŠ›
        self.attn_blks = nn.ModuleList()
        tmp_layer = TransformerDecoderLayer(
            inter_dim,
            nhead=num_heads,
            dim_feedforward=dim_feedforward,
            normalize_before=True,
            dropout=dprate,
            activation=activation,
        )
        for i in range(len(feature_dim_list) - 1):
            self.attn_blks.append(TransformerDecoder(tmp_layer, ca_layers))

        # æ³¨æ„åŠ›æ± åŒ–å’Œ MLP å±‚
        self.attn_pool = TransformerEncoderLayer(
            inter_dim,
            nhead=num_heads,
            dim_feedforward=dim_feedforward,
            normalize_before=True,
            dropout=dprate,
            activation=activation,
        )

        # è¯„åˆ†çº¿æ€§å±‚
        linear_dim = inter_dim
        self.score_linear = nn.Sequential(
            nn.LayerNorm(linear_dim),
            nn.Linear(linear_dim, linear_dim),
            self.act_layer,
            nn.LayerNorm(linear_dim),
            nn.Linear(linear_dim, linear_dim),
            self.act_layer,
            nn.Linear(linear_dim, self.num_class),
            nn.Softmax(dim=-1),  # è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
        )

        # ä½ç½®ç¼–ç 
        self.h_emb = nn.Parameter(torch.randn(1, inter_dim // 2, 32, 1))
        self.w_emb = nn.Parameter(torch.randn(1, inter_dim // 2, 1, 32))

        nn.init.trunc_normal_(self.h_emb.data, std=0.02)
        nn.init.trunc_normal_(self.w_emb.data, std=0.02)
        self._init_linear(self.dim_reduce)
        self._init_linear(self.sa_attn_blks)
        self._init_linear(self.attn_blks)
        self._init_linear(self.attn_pool)

        self.eps = 1e-8

    def _init_linear(self, m):
        for module in m.modules():
            if isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight.data)
                nn.init.constant_(module.bias.data, 0)

    def preprocess(self, x):
        x = (x - self.default_mean.to(x)) / self.default_std.to(x)
        return x

    def fix_bn(self, model):
        for m in model.modules():
            if isinstance(m, nn.BatchNorm2d):
                for p in m.parameters():
                    p.requires_grad = False
                m.eval()

    def forward_cross_attention(self, x):
        # æµ‹è¯•æ—¶å¯é€‰è°ƒæ•´å°ºå¯¸
        if not self.training and self.test_img_size is not None:
            x = TF.resize(x, self.test_img_size, antialias=True)

        x = self.preprocess(x)
        
        # æå–å¤šå°ºåº¦ç‰¹å¾
        dist_feat_list = self.semantic_model(x)
        self.fix_bn(self.semantic_model)
        self.semantic_model.eval()

        start_level = 0
        end_level = len(dist_feat_list)

        b, c, th, tw = dist_feat_list[end_level - 1].shape
        pos_emb = torch.cat(
            (
                self.h_emb.repeat(1, 1, 1, self.w_emb.shape[3]),
                self.w_emb.repeat(1, 1, self.h_emb.shape[2], 1),
            ),
            dim=1,
        )

        token_feat_list = []
        for i in reversed(range(start_level, end_level)):
            tmp_dist_feat = dist_feat_list[i]
            
            # é—¨æ§å±€éƒ¨æ± åŒ–ï¼ˆNR-IQA æ¨¡å¼ï¼‰
            tmp_feat = self.weight_pool[i](tmp_dist_feat)

            if tmp_feat.shape[2] > th and tmp_feat.shape[3] > tw:
                tmp_feat = F.adaptive_avg_pool2d(tmp_feat, (th, tw))

            # è‡ªæ³¨æ„åŠ›
            tmp_pos_emb = F.interpolate(
                pos_emb, size=tmp_feat.shape[2:], mode='bicubic', align_corners=False
            )
            tmp_pos_emb = tmp_pos_emb.flatten(2).permute(2, 0, 1)

            tmp_feat = self.dim_reduce[i](tmp_feat)
            tmp_feat = tmp_feat.flatten(2).permute(2, 0, 1)
            tmp_feat = tmp_feat + tmp_pos_emb

            tmp_feat = self.sa_attn_blks[i](tmp_feat)
            token_feat_list.append(tmp_feat)

        # ä»é«˜å±‚åˆ°ä½å±‚ï¼šç²—åˆ°ç»†
        query = token_feat_list[0]
        for i in range(len(token_feat_list) - 1):
            key_value = token_feat_list[i + 1]
            query = self.attn_blks[i](query, key_value)

        final_feat = self.attn_pool(query)
        out_score = self.score_linear(final_feat.mean(dim=0))

        return out_score

    def forward(self, x, return_mos=True, return_dist=False):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å›¾åƒ (B, 3, H, W)ï¼Œå€¼èŒƒå›´ [0, 1]
            return_mos: è¿”å› MOS åˆ†æ•°
            return_dist: è¿”å›æ¦‚ç‡åˆ†å¸ƒ
            
        Returns:
            MOS åˆ†æ•° (1-10 èŒƒå›´) å’Œ/æˆ– æ¦‚ç‡åˆ†å¸ƒ
        """
        score = self.forward_cross_attention(x)
        mos = dist_to_mos(score)

        return_list = []
        if return_mos:
            return_list.append(mos)
        if return_dist:
            return_list.append(score)

        if len(return_list) > 1:
            return return_list
        else:
            return return_list[0]


def clean_state_dict(state_dict):
    """æ¸…ç† checkpointï¼Œç§»é™¤ .module å‰ç¼€"""
    cleaned_state_dict = OrderedDict()
    for k, v in state_dict.items():
        name = k[7:] if k.startswith('module.') else k
        cleaned_state_dict[name] = v
    return cleaned_state_dict


def load_topiq_weights(model: CFANet, weight_path: str, device: torch.device) -> None:
    """
    åŠ è½½ TOPIQ é¢„è®­ç»ƒæƒé‡
    
    Args:
        model: CFANet æ¨¡å‹å®ä¾‹
        weight_path: æƒé‡æ–‡ä»¶è·¯å¾„
        device: ç›®æ ‡è®¾å¤‡
    """
    if not os.path.exists(weight_path):
        raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weight_path}")
    
    print(f"ğŸ“¥ åŠ è½½ TOPIQ æƒé‡: {os.path.basename(weight_path)}")
    state_dict = torch.load(weight_path, map_location=device, weights_only=False)
    
    # pyiqa æƒé‡æ ¼å¼: {'params': {...}}
    if 'params' in state_dict:
        state_dict = state_dict['params']
    
    state_dict = clean_state_dict(state_dict)
    
    # åŠ è½½æƒé‡ (strict=False å› ä¸ºå¯èƒ½æœ‰ä¸€äº›é¢å¤–çš„ buffer)
    missing, unexpected = model.load_state_dict(state_dict, strict=False)
    
    if missing:
        print(f"  âš ï¸  ç¼ºå¤±çš„é”®: {len(missing)}")
    if unexpected:
        print(f"  âš ï¸  æœªé¢„æœŸçš„é”®: {len(unexpected)}")
    
    print(f"âœ… TOPIQ æƒé‡åŠ è½½å®Œæˆ")


def get_topiq_weight_path():
    """
    è·å– TOPIQ æƒé‡æ–‡ä»¶è·¯å¾„
    
    æ”¯æŒ:
    - PyInstaller æ‰“åŒ…åçš„è·¯å¾„
    - å¼€å‘ç¯å¢ƒçš„ models/ ç›®å½•
    """
    weight_name = 'cfanet_iaa_ava_res50-3cd62bb3.pth'
    
    search_paths = []
    
    if hasattr(sys, '_MEIPASS'):
        search_paths.append(os.path.join(sys._MEIPASS, 'models', weight_name))
    
    base_dir = os.path.dirname(os.path.abspath(__file__))
    search_paths.append(os.path.join(base_dir, 'models', weight_name))
    search_paths.append(os.path.join(base_dir, weight_name))
    
    for path in search_paths:
        if os.path.exists(path):
            return path
    
    raise FileNotFoundError(
        f"TOPIQ æƒé‡æ–‡ä»¶æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿ models/{weight_name} å­˜åœ¨ã€‚\n"
        f"æœç´¢è·¯å¾„: {search_paths}"
    )


class TOPIQScorer:
    """
    TOPIQ ç¾å­¦è¯„åˆ†å™¨å°è£…ç±»
    
    æä¾›ä¸ NIMAScorer å…¼å®¹çš„æ¥å£ï¼Œæ–¹ä¾¿æ›¿æ¢
    """
    
    def __init__(self, device='mps'):
        """
        åˆå§‹åŒ– TOPIQ è¯„åˆ†å™¨
        
        Args:
            device: è®¡ç®—è®¾å¤‡ ('mps', 'cuda', 'cpu')
        """
        self.device = self._get_device(device)
        self._model = None
        
    def _get_device(self, preferred_device='mps'):
        if preferred_device == 'mps':
            try:
                if torch.backends.mps.is_available():
                    return torch.device('mps')
            except:
                pass
        
        if preferred_device == 'cuda' or torch.cuda.is_available():
            return torch.device('cuda')
        
        return torch.device('cpu')
    
    def _load_model(self):
        if self._model is None:
            print(f"ğŸ¨ åˆå§‹åŒ– TOPIQ è¯„åˆ†å™¨ (è®¾å¤‡: {self.device})...")
            weight_path = get_topiq_weight_path()
            
            self._model = CFANet()
            load_topiq_weights(self._model, weight_path, self.device)
            self._model.to(self.device)
            self._model.eval()
            
        return self._model
    
    def calculate_score(self, image_path: str) -> float:
        """
        è®¡ç®— TOPIQ ç¾å­¦è¯„åˆ†
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            
        Returns:
            ç¾å­¦åˆ†æ•° (1-10 èŒƒå›´)
        """
        if not os.path.exists(image_path):
            print(f"âŒ å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
            return None
        
        try:
            model = self._load_model()
            
            # åŠ è½½å›¾ç‰‡
            img = Image.open(image_path).convert('RGB')
            
            # é™åˆ¶å›¾ç‰‡å°ºå¯¸ï¼ˆé¿å…å†…å­˜æº¢å‡ºå’ŒMPSå…¼å®¹æ€§é—®é¢˜ï¼‰
            # ä½¿ç”¨å›ºå®š 384x384 å°ºå¯¸ç¡®ä¿ adaptive_avg_pool2d åœ¨ MPS ä¸Šæ­£å¸¸å·¥ä½œ
            target_size = 384
            img = img.resize((target_size, target_size), Image.LANCZOS)
            
            transform = T.ToTensor()
            img_tensor = transform(img).unsqueeze(0).to(self.device)
            
            # è®¡ç®—è¯„åˆ†
            with torch.no_grad():
                score = model(img_tensor, return_mos=True)
            
            if isinstance(score, torch.Tensor):
                score = score.item()
            
            # æ¢å¤åŸå§‹è¯„åˆ† (ä¸è¿›è¡Œ Scaling)
            return float(max(1.0, min(10.0, score)))
            
        except Exception as e:
            print(f"âŒ TOPIQ è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=" * 70)
    print("TOPIQ ç‹¬ç«‹æ¨¡å‹æµ‹è¯•")
    print("=" * 70)
    
    scorer = TOPIQScorer(device='mps')
    
    test_image = "img/_Z9W0960.jpg"
    
    if os.path.exists(test_image):
        print(f"\nğŸ“· æµ‹è¯•å›¾ç‰‡: {test_image}")
        
        import time
        start = time.time()
        score = scorer.calculate_score(test_image)
        elapsed = time.time() - start
        
        if score is not None:
            print(f"   âœ… TOPIQ åˆ†æ•°: {score:.2f} / 10")
            print(f"   â±ï¸  è€—æ—¶: {elapsed*1000:.0f}ms")
        else:
            print(f"   âŒ TOPIQ è®¡ç®—å¤±è´¥")
    else:
        print(f"\nâš ï¸  æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨: {test_image}")
    
    print("\n" + "=" * 70)
